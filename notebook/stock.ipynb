{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stock symbols from CSV\n",
    "csv_file = \"stock_symbol.csv\"\n",
    "symbols_df = pd.read_csv(csv_file)\n",
    "\n",
    "# Validate CSV data\n",
    "if 'Symbol' not in symbols_df.columns:\n",
    "    raise ValueError(\"CSV file must contain a 'Symbol' column.\")\n",
    "\n",
    "stock_symbols = symbols_df['Symbol'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  6 of 6 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Date</th>\n",
       "      <th colspan=\"6\" halign=\"left\">Close</th>\n",
       "      <th colspan=\"3\" halign=\"left\">High</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Open</th>\n",
       "      <th colspan=\"6\" halign=\"left\">Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NFLX</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>...</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NFLX</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NFLX</th>\n",
       "      <th>TSLA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>6.440332</td>\n",
       "      <td>6.695000</td>\n",
       "      <td>15.536651</td>\n",
       "      <td>23.254051</td>\n",
       "      <td>7.640000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.455078</td>\n",
       "      <td>6.830500</td>\n",
       "      <td>15.605068</td>\n",
       "      <td>...</td>\n",
       "      <td>15.541608</td>\n",
       "      <td>23.006108</td>\n",
       "      <td>7.931429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>493729600</td>\n",
       "      <td>151998000</td>\n",
       "      <td>78541293</td>\n",
       "      <td>38409100</td>\n",
       "      <td>17239600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>6.451466</td>\n",
       "      <td>6.734500</td>\n",
       "      <td>15.468233</td>\n",
       "      <td>23.261564</td>\n",
       "      <td>7.358571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.487879</td>\n",
       "      <td>6.774000</td>\n",
       "      <td>15.563671</td>\n",
       "      <td>...</td>\n",
       "      <td>15.547310</td>\n",
       "      <td>23.178918</td>\n",
       "      <td>7.652857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>601904800</td>\n",
       "      <td>177038000</td>\n",
       "      <td>120638494</td>\n",
       "      <td>49749600</td>\n",
       "      <td>23753100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>6.348846</td>\n",
       "      <td>6.612500</td>\n",
       "      <td>15.078297</td>\n",
       "      <td>23.118816</td>\n",
       "      <td>7.617143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.477045</td>\n",
       "      <td>6.736500</td>\n",
       "      <td>15.514587</td>\n",
       "      <td>...</td>\n",
       "      <td>15.514587</td>\n",
       "      <td>23.201463</td>\n",
       "      <td>7.361429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>552160000</td>\n",
       "      <td>143576000</td>\n",
       "      <td>159744526</td>\n",
       "      <td>58182400</td>\n",
       "      <td>23290400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>6.337111</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>14.727282</td>\n",
       "      <td>22.878378</td>\n",
       "      <td>7.485714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.379844</td>\n",
       "      <td>6.616000</td>\n",
       "      <td>15.121431</td>\n",
       "      <td>...</td>\n",
       "      <td>15.106557</td>\n",
       "      <td>23.013618</td>\n",
       "      <td>7.731429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>477131200</td>\n",
       "      <td>220604000</td>\n",
       "      <td>257533695</td>\n",
       "      <td>50559700</td>\n",
       "      <td>9955400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>6.379240</td>\n",
       "      <td>6.676000</td>\n",
       "      <td>14.923613</td>\n",
       "      <td>23.036160</td>\n",
       "      <td>7.614286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.379842</td>\n",
       "      <td>6.684000</td>\n",
       "      <td>14.954103</td>\n",
       "      <td>...</td>\n",
       "      <td>14.675224</td>\n",
       "      <td>22.750650</td>\n",
       "      <td>7.498571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>447610800</td>\n",
       "      <td>196610000</td>\n",
       "      <td>189680313</td>\n",
       "      <td>51197400</td>\n",
       "      <td>8180900</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>222.130005</td>\n",
       "      <td>190.259995</td>\n",
       "      <td>156.229996</td>\n",
       "      <td>375.390015</td>\n",
       "      <td>932.530029</td>\n",
       "      <td>259.160004</td>\n",
       "      <td>225.619995</td>\n",
       "      <td>191.330002</td>\n",
       "      <td>157.130005</td>\n",
       "      <td>...</td>\n",
       "      <td>154.809998</td>\n",
       "      <td>372.540009</td>\n",
       "      <td>920.530029</td>\n",
       "      <td>249.309998</td>\n",
       "      <td>65299300</td>\n",
       "      <td>63547600</td>\n",
       "      <td>33591600</td>\n",
       "      <td>35184700</td>\n",
       "      <td>4634100</td>\n",
       "      <td>134008900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3834</th>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>223.190002</td>\n",
       "      <td>192.169998</td>\n",
       "      <td>158.880005</td>\n",
       "      <td>382.190002</td>\n",
       "      <td>928.380005</td>\n",
       "      <td>268.459991</td>\n",
       "      <td>223.679993</td>\n",
       "      <td>193.929993</td>\n",
       "      <td>160.080002</td>\n",
       "      <td>...</td>\n",
       "      <td>155.300003</td>\n",
       "      <td>374.649994</td>\n",
       "      <td>927.500000</td>\n",
       "      <td>263.799988</td>\n",
       "      <td>36412700</td>\n",
       "      <td>41267300</td>\n",
       "      <td>20111400</td>\n",
       "      <td>19689500</td>\n",
       "      <td>3520000</td>\n",
       "      <td>146486900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3835</th>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>223.889999</td>\n",
       "      <td>196.009995</td>\n",
       "      <td>158.860001</td>\n",
       "      <td>382.140015</td>\n",
       "      <td>935.520020</td>\n",
       "      <td>282.760010</td>\n",
       "      <td>225.190002</td>\n",
       "      <td>198.339996</td>\n",
       "      <td>160.274994</td>\n",
       "      <td>...</td>\n",
       "      <td>156.960007</td>\n",
       "      <td>377.970001</td>\n",
       "      <td>923.000000</td>\n",
       "      <td>254.600006</td>\n",
       "      <td>35905900</td>\n",
       "      <td>53679200</td>\n",
       "      <td>17113300</td>\n",
       "      <td>16092600</td>\n",
       "      <td>3256900</td>\n",
       "      <td>212787800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3836</th>\n",
       "      <td>2025-04-03</td>\n",
       "      <td>203.190002</td>\n",
       "      <td>178.410004</td>\n",
       "      <td>152.630005</td>\n",
       "      <td>373.109985</td>\n",
       "      <td>917.049988</td>\n",
       "      <td>267.279999</td>\n",
       "      <td>207.490005</td>\n",
       "      <td>184.130005</td>\n",
       "      <td>154.686996</td>\n",
       "      <td>...</td>\n",
       "      <td>152.835007</td>\n",
       "      <td>374.790009</td>\n",
       "      <td>901.799988</td>\n",
       "      <td>265.290009</td>\n",
       "      <td>103419000</td>\n",
       "      <td>95553600</td>\n",
       "      <td>28416100</td>\n",
       "      <td>30198000</td>\n",
       "      <td>5864600</td>\n",
       "      <td>136174300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3837</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>188.380005</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>147.740005</td>\n",
       "      <td>359.839996</td>\n",
       "      <td>855.859985</td>\n",
       "      <td>239.429993</td>\n",
       "      <td>199.880005</td>\n",
       "      <td>178.139999</td>\n",
       "      <td>153.089996</td>\n",
       "      <td>...</td>\n",
       "      <td>149.899994</td>\n",
       "      <td>364.130005</td>\n",
       "      <td>896.500000</td>\n",
       "      <td>255.380005</td>\n",
       "      <td>125569000</td>\n",
       "      <td>122951300</td>\n",
       "      <td>39786000</td>\n",
       "      <td>49138700</td>\n",
       "      <td>6783900</td>\n",
       "      <td>180324400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3838 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price        Date       Close                                                  \\\n",
       "Ticker                   AAPL        AMZN        GOOG        MSFT        NFLX   \n",
       "0      2010-01-04    6.440332    6.695000   15.536651   23.254051    7.640000   \n",
       "1      2010-01-05    6.451466    6.734500   15.468233   23.261564    7.358571   \n",
       "2      2010-01-06    6.348846    6.612500   15.078297   23.118816    7.617143   \n",
       "3      2010-01-07    6.337111    6.500000   14.727282   22.878378    7.485714   \n",
       "4      2010-01-08    6.379240    6.676000   14.923613   23.036160    7.614286   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "3833   2025-03-31  222.130005  190.259995  156.229996  375.390015  932.530029   \n",
       "3834   2025-04-01  223.190002  192.169998  158.880005  382.190002  928.380005   \n",
       "3835   2025-04-02  223.889999  196.009995  158.860001  382.140015  935.520020   \n",
       "3836   2025-04-03  203.190002  178.410004  152.630005  373.109985  917.049988   \n",
       "3837   2025-04-04  188.380005  171.000000  147.740005  359.839996  855.859985   \n",
       "\n",
       "Price                     High                          ...        Open  \\\n",
       "Ticker        TSLA        AAPL        AMZN        GOOG  ...        GOOG   \n",
       "0              NaN    6.455078    6.830500   15.605068  ...   15.541608   \n",
       "1              NaN    6.487879    6.774000   15.563671  ...   15.547310   \n",
       "2              NaN    6.477045    6.736500   15.514587  ...   15.514587   \n",
       "3              NaN    6.379844    6.616000   15.121431  ...   15.106557   \n",
       "4              NaN    6.379842    6.684000   14.954103  ...   14.675224   \n",
       "...            ...         ...         ...         ...  ...         ...   \n",
       "3833    259.160004  225.619995  191.330002  157.130005  ...  154.809998   \n",
       "3834    268.459991  223.679993  193.929993  160.080002  ...  155.300003   \n",
       "3835    282.760010  225.190002  198.339996  160.274994  ...  156.960007   \n",
       "3836    267.279999  207.490005  184.130005  154.686996  ...  152.835007   \n",
       "3837    239.429993  199.880005  178.139999  153.089996  ...  149.899994   \n",
       "\n",
       "Price                                          Volume                        \\\n",
       "Ticker        MSFT        NFLX        TSLA       AAPL       AMZN       GOOG   \n",
       "0        23.006108    7.931429         NaN  493729600  151998000   78541293   \n",
       "1        23.178918    7.652857         NaN  601904800  177038000  120638494   \n",
       "2        23.201463    7.361429         NaN  552160000  143576000  159744526   \n",
       "3        23.013618    7.731429         NaN  477131200  220604000  257533695   \n",
       "4        22.750650    7.498571         NaN  447610800  196610000  189680313   \n",
       "...            ...         ...         ...        ...        ...        ...   \n",
       "3833    372.540009  920.530029  249.309998   65299300   63547600   33591600   \n",
       "3834    374.649994  927.500000  263.799988   36412700   41267300   20111400   \n",
       "3835    377.970001  923.000000  254.600006   35905900   53679200   17113300   \n",
       "3836    374.790009  901.799988  265.290009  103419000   95553600   28416100   \n",
       "3837    364.130005  896.500000  255.380005  125569000  122951300   39786000   \n",
       "\n",
       "Price                                    \n",
       "Ticker      MSFT      NFLX         TSLA  \n",
       "0       38409100  17239600          NaN  \n",
       "1       49749600  23753100          NaN  \n",
       "2       58182400  23290400          NaN  \n",
       "3       50559700   9955400          NaN  \n",
       "4       51197400   8180900          NaN  \n",
       "...          ...       ...          ...  \n",
       "3833    35184700   4634100  134008900.0  \n",
       "3834    19689500   3520000  146486900.0  \n",
       "3835    16092600   3256900  212787800.0  \n",
       "3836    30198000   5864600  136174300.0  \n",
       "3837    49138700   6783900  180324400.0  \n",
       "\n",
       "[3838 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_date = \"2010-01-01\"\n",
    "end_date = pd.Timestamp.today()\n",
    "data = yf.download(stock_symbols, start = start_date, end = end_date)\n",
    "data.reset_index(inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing stock: GOOG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 3838\n",
      "Training Data shape: (3070, 2)\n",
      "Testing Data shape: (768, 2)\n",
      "Processing stock: AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 3838\n",
      "Training Data shape: (3070, 2)\n",
      "Testing Data shape: (768, 2)\n",
      "Processing stock: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 3838\n",
      "Training Data shape: (3070, 2)\n",
      "Testing Data shape: (768, 2)\n",
      "Processing stock: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 3838\n",
      "Training Data shape: (3070, 2)\n",
      "Testing Data shape: (768, 2)\n",
      "Processing stock: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 3716\n",
      "Training Data shape: (2972, 2)\n",
      "Testing Data shape: (744, 2)\n",
      "Processing stock: NFLX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 3838\n",
      "Training Data shape: (3070, 2)\n",
      "Testing Data shape: (768, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results for each stock\n",
    "predictions_dict = {}\n",
    "\n",
    "for stock_symbol in stock_symbols:\n",
    "    print(f\"Processing stock: {stock_symbol}\")\n",
    "    data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "    data.reset_index(inplace=True)\n",
    "\n",
    "    # Determine the cutoff point (80% for training, 20% for testing)\n",
    "    cutoff = int(len(data) * 0.8)\n",
    "\n",
    "    # Step 1: Prepare the training and testing data using iloc\n",
    "    data_train = data.iloc[:cutoff][['Open', 'Close']].copy()\n",
    "    data_test = data.iloc[cutoff:][['Open', 'Close']].copy()\n",
    "\n",
    "    # Display the shapes of both datasets\n",
    "    print(f\"Total data points: {len(data)}\")\n",
    "    print(f\"Training Data shape: {data_train.shape}\")\n",
    "    print(f\"Testing Data shape: {data_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_train_scaled = scaler.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare the input (x) and output (y) for the LSTM model\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# Create sequences of data\n",
    "time_steps = 60  # Number of time steps to look back\n",
    "\n",
    "for i in range(time_steps, data_train_scaled.shape[0]):\n",
    "    x.append(data_train_scaled[i - time_steps:i])  # Previous 60 time steps\n",
    "    y.append(data_train_scaled[i])  # Current Open and Close prices\n",
    "\n",
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(x.shape[1], x.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=60, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(LSTM(units=80, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(units=120, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer to predict both Open and Close prices\n",
    "model.add(Dense(units=2))  # 2 units for Open and Close prices\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "95/95 [==============================] - 21s 144ms/step - loss: 0.0266\n",
      "Epoch 2/100\n",
      "95/95 [==============================] - 16s 165ms/step - loss: 0.0068\n",
      "Epoch 3/100\n",
      "95/95 [==============================] - 15s 159ms/step - loss: 0.0057\n",
      "Epoch 4/100\n",
      "95/95 [==============================] - 15s 157ms/step - loss: 0.0050\n",
      "Epoch 5/100\n",
      "95/95 [==============================] - 15s 157ms/step - loss: 0.0047\n",
      "Epoch 6/100\n",
      "95/95 [==============================] - 15s 154ms/step - loss: 0.0044\n",
      "Epoch 7/100\n",
      "95/95 [==============================] - 15s 160ms/step - loss: 0.0041\n",
      "Epoch 8/100\n",
      "95/95 [==============================] - 15s 156ms/step - loss: 0.0043\n",
      "Epoch 9/100\n",
      "95/95 [==============================] - 15s 155ms/step - loss: 0.0036\n",
      "Epoch 10/100\n",
      "95/95 [==============================] - 15s 157ms/step - loss: 0.0037\n",
      "Epoch 11/100\n",
      "95/95 [==============================] - 16s 167ms/step - loss: 0.0037\n",
      "Epoch 12/100\n",
      "95/95 [==============================] - 17s 181ms/step - loss: 0.0039\n",
      "Epoch 13/100\n",
      "95/95 [==============================] - 18s 185ms/step - loss: 0.0035\n",
      "Epoch 14/100\n",
      "95/95 [==============================] - 33s 342ms/step - loss: 0.0032\n",
      "Epoch 15/100\n",
      "95/95 [==============================] - 33s 343ms/step - loss: 0.0032\n",
      "Epoch 16/100\n",
      "95/95 [==============================] - 22s 234ms/step - loss: 0.0032\n",
      "Epoch 17/100\n",
      "95/95 [==============================] - 17s 177ms/step - loss: 0.0027\n",
      "Epoch 18/100\n",
      "95/95 [==============================] - 24s 258ms/step - loss: 0.0030\n",
      "Epoch 19/100\n",
      "95/95 [==============================] - 32s 342ms/step - loss: 0.0029\n",
      "Epoch 20/100\n",
      "95/95 [==============================] - 31s 330ms/step - loss: 0.0025\n",
      "Epoch 21/100\n",
      "95/95 [==============================] - 31s 329ms/step - loss: 0.0028\n",
      "Epoch 22/100\n",
      "95/95 [==============================] - 24s 250ms/step - loss: 0.0026\n",
      "Epoch 23/100\n",
      "95/95 [==============================] - 14s 148ms/step - loss: 0.0026\n",
      "Epoch 24/100\n",
      "95/95 [==============================] - 15s 161ms/step - loss: 0.0025\n",
      "Epoch 25/100\n",
      "95/95 [==============================] - 14s 151ms/step - loss: 0.0027\n",
      "Epoch 26/100\n",
      "95/95 [==============================] - 14s 146ms/step - loss: 0.0027\n",
      "Epoch 27/100\n",
      "95/95 [==============================] - 14s 143ms/step - loss: 0.0027\n",
      "Epoch 28/100\n",
      "95/95 [==============================] - 14s 151ms/step - loss: 0.0025\n",
      "Epoch 29/100\n",
      "95/95 [==============================] - 14s 144ms/step - loss: 0.0024\n",
      "Epoch 30/100\n",
      "95/95 [==============================] - 14s 143ms/step - loss: 0.0025\n",
      "Epoch 31/100\n",
      "95/95 [==============================] - 13s 142ms/step - loss: 0.0024\n",
      "Epoch 32/100\n",
      "95/95 [==============================] - 13s 140ms/step - loss: 0.0023\n",
      "Epoch 33/100\n",
      "95/95 [==============================] - 13s 140ms/step - loss: 0.0025\n",
      "Epoch 34/100\n",
      "95/95 [==============================] - 14s 145ms/step - loss: 0.0023\n",
      "Epoch 35/100\n",
      "95/95 [==============================] - 23s 247ms/step - loss: 0.0025\n",
      "Epoch 36/100\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.0025\n",
      "Epoch 37/100\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.0025\n",
      "Epoch 38/100\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.0025\n",
      "Epoch 39/100\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.0023\n",
      "Epoch 40/100\n",
      "95/95 [==============================] - 29s 310ms/step - loss: 0.0027\n",
      "Epoch 41/100\n",
      "95/95 [==============================] - 29s 304ms/step - loss: 0.0024\n",
      "Epoch 42/100\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.0029\n",
      "Epoch 43/100\n",
      "95/95 [==============================] - 30s 316ms/step - loss: 0.0023\n",
      "Epoch 44/100\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.0023\n",
      "Epoch 45/100\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.0023\n",
      "Epoch 46/100\n",
      "95/95 [==============================] - 29s 308ms/step - loss: 0.0024\n",
      "Epoch 47/100\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.0022\n",
      "Epoch 48/100\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.0024\n",
      "Epoch 49/100\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.0021\n",
      "Epoch 50/100\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.0023\n",
      "Epoch 51/100\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.0024\n",
      "Epoch 52/100\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.0023\n",
      "Epoch 53/100\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.0024\n",
      "Epoch 54/100\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.0023\n",
      "Epoch 55/100\n",
      "95/95 [==============================] - 30s 318ms/step - loss: 0.0026\n",
      "Epoch 56/100\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 0.0022\n",
      "Epoch 57/100\n",
      "95/95 [==============================] - 29s 309ms/step - loss: 0.0022\n",
      "Epoch 58/100\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.0025\n",
      "Epoch 59/100\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.0023\n",
      "Epoch 60/100\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.0025\n",
      "Epoch 61/100\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.0023\n",
      "Epoch 62/100\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.0023\n",
      "Epoch 63/100\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.0021\n",
      "Epoch 64/100\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.0023\n",
      "Epoch 65/100\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.0023\n",
      "Epoch 66/100\n",
      "95/95 [==============================] - 27s 283ms/step - loss: 0.0023\n",
      "Epoch 67/100\n",
      "95/95 [==============================] - 23s 243ms/step - loss: 0.0022\n",
      "Epoch 68/100\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.0021\n",
      "Epoch 69/100\n",
      "95/95 [==============================] - 30s 317ms/step - loss: 0.0021\n",
      "Epoch 70/100\n",
      "95/95 [==============================] - 30s 312ms/step - loss: 0.0024\n",
      "Epoch 71/100\n",
      "95/95 [==============================] - 20s 208ms/step - loss: 0.0023\n",
      "Epoch 72/100\n",
      "95/95 [==============================] - 13s 134ms/step - loss: 0.0024\n",
      "Epoch 73/100\n",
      "95/95 [==============================] - 13s 139ms/step - loss: 0.0022\n",
      "Epoch 74/100\n",
      "95/95 [==============================] - 17s 182ms/step - loss: 0.0023\n",
      "Epoch 75/100\n",
      "95/95 [==============================] - 19s 205ms/step - loss: 0.0022\n",
      "Epoch 76/100\n",
      "95/95 [==============================] - 19s 196ms/step - loss: 0.0022\n",
      "Epoch 77/100\n",
      "95/95 [==============================] - 29s 306ms/step - loss: 0.0022\n",
      "Epoch 78/100\n",
      "95/95 [==============================] - 26s 278ms/step - loss: 0.0022\n",
      "Epoch 79/100\n",
      "95/95 [==============================] - 26s 278ms/step - loss: 0.0023\n",
      "Epoch 80/100\n",
      "95/95 [==============================] - 29s 301ms/step - loss: 0.0022\n",
      "Epoch 81/100\n",
      "95/95 [==============================] - 20s 214ms/step - loss: 0.0023\n",
      "Epoch 82/100\n",
      "95/95 [==============================] - 26s 271ms/step - loss: 0.0022\n",
      "Epoch 83/100\n",
      "95/95 [==============================] - 28s 296ms/step - loss: 0.0022\n",
      "Epoch 84/100\n",
      "95/95 [==============================] - 27s 287ms/step - loss: 0.0022\n",
      "Epoch 85/100\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.0022\n",
      "Epoch 86/100\n",
      "95/95 [==============================] - 14s 149ms/step - loss: 0.0021\n",
      "Epoch 87/100\n",
      "95/95 [==============================] - 13s 133ms/step - loss: 0.0021\n",
      "Epoch 88/100\n",
      "95/95 [==============================] - 13s 141ms/step - loss: 0.0023\n",
      "Epoch 89/100\n",
      "95/95 [==============================] - 13s 133ms/step - loss: 0.0021\n",
      "Epoch 90/100\n",
      "95/95 [==============================] - 12s 124ms/step - loss: 0.0021\n",
      "Epoch 91/100\n",
      "95/95 [==============================] - 21s 222ms/step - loss: 0.0024\n",
      "Epoch 92/100\n",
      "95/95 [==============================] - 29s 307ms/step - loss: 0.0021\n",
      "Epoch 93/100\n",
      "95/95 [==============================] - 29s 309ms/step - loss: 0.0023\n",
      "Epoch 94/100\n",
      "95/95 [==============================] - 28s 298ms/step - loss: 0.0022\n",
      "Epoch 95/100\n",
      "95/95 [==============================] - 26s 270ms/step - loss: 0.0020\n",
      "Epoch 96/100\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.0023\n",
      "Epoch 97/100\n",
      "95/95 [==============================] - 28s 299ms/step - loss: 0.0021\n",
      "Epoch 98/100\n",
      "95/95 [==============================] - 28s 298ms/step - loss: 0.0021\n",
      "Epoch 99/100\n",
      "95/95 [==============================] - 18s 183ms/step - loss: 0.0022\n",
      "Epoch 100/100\n",
      "95/95 [==============================] - 13s 137ms/step - loss: 0.0024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2300ef2d420>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Train the model\n",
    "model.fit(x, y, epochs=100, batch_size=32)  # Adjust epochs and batch size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Prepare the test data (you can do this in a similar way)\n",
    "data_test_scaled = scaler.transform(data_test)\n",
    "\n",
    "x_test = []\n",
    "for i in range(time_steps, data_test_scaled.shape[0]):\n",
    "    x_test.append(data_test_scaled[i - time_steps:i])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 2s 50ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Make predictions\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Inverse transform the predictions to get actual prices\n",
    "predictions_inverse = scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_inverse will contain two columns: [Predicted Open, Predicted Close]\n",
    "predicted_open_price = predictions_inverse[:, 0]\n",
    "predicted_close_price = predictions_inverse[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predicted prices in the dictionary\n",
    "predictions_dict[stock_symbol] = {\n",
    "    \"Predicted_Open_Prices\": predicted_open_price,\n",
    "    \"Predicted_Close_Prices\": predicted_close_price\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for each stock\n",
    "model.save(f'stock_price_model_GOOG.h5')\n",
    "model.save(f'stock_price_model_AAPL.h5')\n",
    "model.save(f'stock_price_model_MSFT.h5')\n",
    "model.save(f'stock_price_model_AMZN.h5')\n",
    "model.save(f'stock_price_model_TSLA.h5')\n",
    "model.save(f'stock_price_model_NFLX.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models trained and predictions saved.\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to a CSV file\n",
    "for symbol, prediction in predictions_dict.items():\n",
    "    df = pd.DataFrame({\n",
    "        'Predicted_Open': prediction['Predicted_Open_Prices'],\n",
    "        'Predicted_Close': prediction['Predicted_Close_Prices']\n",
    "    })\n",
    "    df.to_csv(f'predictions_GOOG.csv', index=False)\n",
    "    df.to_csv(f'predictions_AAPL.csv', index=False)\n",
    "    df.to_csv(f'predictions_MSFT.csv', index=False)\n",
    "    df.to_csv(f'predictions_AMZN.csv', index=False)\n",
    "    df.to_csv(f'predictions_TSLA.csv', index=False)\n",
    "    df.to_csv(f'predictions_NFLX.csv', index=False)\n",
    "\n",
    "print(\"All models trained and predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
